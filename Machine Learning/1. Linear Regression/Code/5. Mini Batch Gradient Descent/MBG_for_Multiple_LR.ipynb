{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "90c8c96d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.datasets import load_diabetes\n",
    "from sklearn.metrics import r2_score"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3fa68f5",
   "metadata": {},
   "source": [
    "#                                              Introduction (Why needed Mini Batch Gradient Descent?)\n",
    "```js\n",
    "        The Mini Batch Gradient Descent is used here is for Multiple Linear Regression where we have m Input Columns and obviously 1 Output Column. To understand this Gradient Descent, first have a CONSPICUOUS GRASP on 'Batch Gradient Descent' i.e. the Vanilla Gradient Descent.\n",
    "        \n",
    "        Inside an epoch or each epoch :\n",
    "                i)  'Batch Gradient Descent' : Update ALL THE COEFFICIENTS(B1, B2 ... Bm) and INTERCEPT(B0) after traversing the whole Input Dataset(x_train, y_train).\n",
    "                ii) 'Stochastic Gradient Descent' : Update ALL THE COEFFICIENTS(B1, B2 ... Bm) and INTERCEPT(B0) for \"A RANDOM ROW of the input dataset(x_train, y_train)\" by manually traversing the input dataset using loop.\n",
    "\n",
    "                iii) 'Mini Batch Gradient Descent' : Update ALL THE COEFFICIENTS(B1, B2 ... Bm) and INTERCEPT(B0) for \"SPECIFIC NUMBER RANDOM ROWS of the input dataset(x_train, y_train)\" by manually traversing the input dataset using loop.\n",
    "                      For example, batch_size = 10 and total row numbers = 353. int(353 / 10) = 35. So we can have total 35 batches from 353 rows and we will run the nested loop for 35 batches and each time we randomly pick 10 indexes since batch_size = 10. Now for the random 10 indexes :\n",
    "\n",
    "                                Assume x_train_subset = x_train[:10] and y_train_subset = y_train[:10]\n",
    "                                Now we will update all the COEFFICIENTS(B1, B2 ... Bm) and INTERCEPT(B0) for input dataset i.e. \"x_train_subset and y_train_subset\".\n",
    "                                \n",
    "                                Code :\n",
    "                                ------\n",
    "                                for _ in range(self.epochs):\n",
    "                                    batch_numbers = x_train.shape[0] // self.batch_size\n",
    "                                    for _ in  range(batch_numbers):\n",
    "                                        row_indexes = np.random.randint(low = 0, high = x_train.shape[0], size = self.batch_size)\n",
    "                                        // Now update COEFFICIENTS and INTERCEPT for x_train[row_indexes] and y_train[row_indexes].\n",
    "        \n",
    "        Purpose :\n",
    "        ---------\n",
    "        1) Minimizing the Loss Function finding the 'minima'. In our case, minimizing the Linear Regression''s Loss Function (MSE) finding the 'minima'.\n",
    "\n",
    "        2) Watch [https://youtu.be/_scscQ4HVTY?si=WHZmMD5Eoq_TmEze&t=1001] from 16:41 to 19:00.\n",
    "\n",
    "        3) From Medium.com (https://medium.com/data-science/batch-mini-batch-stochastic-gradient-descent-7a62ecba642a) :\n",
    "           Batch Gradient Descent can be used for smoother curves. SGD can be used when the dataset is large. Batch Gradient Descent converges directly to minima. SGD converges faster for larger datasets. But, since in SGD we use only one example at a time, we cannot implement the vectorized implementation on it. This can slow down the computations. To tackle this problem, a mixture of Batch Gradient Descent and SGD is used.\n",
    "           Neither we use all the dataset all at once nor we use the single example at a time. We use a batch of a fixed number of training examples which is less than the actual dataset and call it a mini-batch. Doing this helps us achieve the advantages of both the former variants we saw.\n",
    "        \n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52969ba7",
   "metadata": {},
   "source": [
    "#                                                     Code Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0c23f0fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Coefficients = \n",
      "[  -9.15865318 -205.45432163  516.69374454  340.61999905 -895.5520019\n",
      "  561.22067904  153.89310954  126.73139688  861.12700152   52.42112238], \n",
      "\n",
      "Intercept = 151.88331005254167\n",
      "\n",
      "Predictions for the First 5 values = \n",
      "[154.1213881  204.81835118 124.93755353 106.08950893 258.5348576 ]\n",
      "\n",
      "r2 score = 0.4399338661568968\n"
     ]
    }
   ],
   "source": [
    "X, Y = load_diabetes(return_X_y=True) # returns np.ndarray. X.shape = (442, 10), Y.shape = (442,).\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(X, Y, test_size=0.2, random_state=2) # (353, 10), (89, 10), (353,), (89,).\n",
    "\n",
    "lr = LinearRegression()\n",
    "lr.fit(X = x_train, y = y_train) # calculated m, b for the future prediction.\n",
    "m, b = lr.coef_, lr.intercept_\n",
    "\n",
    "print(f\"Coefficients = \\n{m}, \\n\\nIntercept = {b}\\n\")\n",
    "\n",
    "print(f\"Predictions for the First 5 values = \\n{lr.predict(x_test[:5])}\\n\")\n",
    "\n",
    "print(f\"r2 score = {r2_score(y_true=y_test, y_pred=lr.predict(x_test))}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b32f78ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Coefficients = \n",
      "[  -8.81335946 -202.79510928  522.19250671  335.51992698 -873.33140848\n",
      "  537.04111143  139.30591763  122.78211934  852.1468558    51.7089542 ], \n",
      "\n",
      "Intercept = 158.30531159558737\n",
      "\n",
      "Predictions for the First 5 values = \n",
      "[159.74171465 210.95121661 131.50046921 111.98095187 265.69711083]\n",
      "\n",
      "r2 score = 0.43558425148402025\n"
     ]
    }
   ],
   "source": [
    "class MBGDRegressor:\n",
    "    def __init__(self, learning_rate: float, epochs: int, batch_size: int):\n",
    "        self.coef_ = None\n",
    "        self.intercept_ = None\n",
    "        self.learning_rate = learning_rate\n",
    "        self.epochs = epochs\n",
    "        self.batch_size = batch_size\n",
    "    \n",
    "    def fit(self, x_train: np.ndarray, y_train: np.ndarray):\n",
    "        B0 = 0 # -> Intercept.\n",
    "        B = np.ones(shape = (x_train.shape[1])) # Coefficient -> B1, B2 ... B10 = [1, 1 ... 1], (10,) -> 1D Vertical Array.\n",
    "        n = x_train.shape[0] # 353, Row numbers.\n",
    "\n",
    "        for _ in range(self.epochs):\n",
    "            batch_numbers = x_train.shape[0] // self.batch_size\n",
    "            \n",
    "            for _ in  range(batch_numbers):\n",
    "                row_indexes = np.random.randint(low = 0, high = x_train.shape[0], size = self.batch_size) # (batch_size,).\n",
    "                y_hat = B0 + np.dot(x_train[row_indexes], B) # dot( (batch_size, 10), (10,) ) = (batch_size).\n",
    "                slope_B0 = (-2 / n) * np.sum(y_train[row_indexes] - y_hat) # np.sum((batch_size)) -> Scaler Value.\n",
    "                slope_Bs = (-2 / n) * np.dot(y_train[row_indexes] - y_hat, x_train[row_indexes])\n",
    "                #                     np.dot((batch_size), (batch_size, 10)) = (10,) shape.\n",
    "\n",
    "                B0 = B0 - self.learning_rate * slope_B0 # Scaler Value.\n",
    "                B  = B  - self.learning_rate * slope_Bs # (10,) shape.\n",
    "\n",
    "        self.intercept_, self.coef_ = B0, B # Scaler Value, (10,) shape.\n",
    "        print(f\"Coefficients = \\n{self.coef_}, \\n\\nIntercept = {self.intercept_}\\n\")\n",
    "    \n",
    "    def predict(self, x_test: np.ndarray):\n",
    "        return np.dot(x_test, self.coef_) + self.intercept_ # np.dot((k, 10), (10)) = (k).\n",
    "\n",
    "def main():\n",
    "    mbgdr = MBGDRegressor(learning_rate = 0.9, epochs = 1_00000, batch_size = 10)\n",
    "    mbgdr.fit(x_train, y_train)\n",
    "\n",
    "    print(f\"Predictions for the First 5 values = \\n{mbgdr.predict(x_test[:5])}\\n\")\n",
    "\n",
    "    print(f\"r2 score = {r2_score(y_true=y_test, y_pred=mbgdr.predict(x_test))}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n",
    "\n",
    "explanation = \"\"\"\n",
    "Sklearn's Linear Regression Model returned in above codeblock :   (Compare sklearn's output with our output)\n",
    "----------------------------------------------------------------------------------\n",
    "Coefficients = \n",
    "[  -9.15865318 -205.45432163  516.69374454  340.61999905 -895.5520019\n",
    "  561.22067904  153.89310954  126.73139688  861.12700152   52.42112238],\n",
    "\n",
    "Intercept = 151.88331005254167\n",
    "\n",
    "Predictions for the First 5 values = \n",
    "[154.1213881  204.81835118 124.93755353 106.08950893 258.5348576 ]\n",
    "\n",
    "r2 score = 0.4399338661568968\n",
    "\n",
    "-----------------------------------------------------------------------------------\n",
    "Sklearn's output and our output are almost close in most of the values. Anyway its not like we will implement this from scratch\n",
    "since we will use sklearn(below).\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30bec795",
   "metadata": {},
   "source": [
    "#                                       Sklearn's Mini Batch Gradient Descent\n",
    "```js\n",
    "    Sklearn doesn''t have direct algo for this but we can do this using SGDRegressor. How? Watch [https://youtu.be/_scscQ4HVTY?si=cdvJ8TJj-q2Mf78E&t=1142] from where this link has started.\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "bbb3693a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.datasets import load_diabetes\n",
    "from sklearn.metrics import r2_score\n",
    "\n",
    "from sklearn.linear_model import SGDRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "44fa7546",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Coefficients = \n",
      "[   5.9427603  -186.60169712  481.94172852  347.38067392  -47.22740426\n",
      " -120.78600566 -210.89844745  112.66268496  524.1436238    80.7130849 ], \n",
      "\n",
      "Intercept = [149.69788186]\n",
      "\n",
      "Predictions for the First 5 values = \n",
      "[148.28557345 198.85592977 123.38942604  99.69895936 270.25731618]\n",
      "\n",
      "r2 score = 0.4422891772020714\n"
     ]
    }
   ],
   "source": [
    "X, Y = load_diabetes(return_X_y=True) # returns np.ndarray. X.shape = (442, 10), Y.shape = (442,).\n",
    "x_train, x_test, y_train, y_test = train_test_split(X, Y, test_size=0.2, random_state=2) # (353, 10), (89, 10), (353,), (89,).\n",
    "\n",
    "epochs, batch_size = 500, 20\n",
    "mbgd = SGDRegressor(loss = 'squared_error', learning_rate = 'constant', eta0 = 0.2) # 'max_iter' parameter is not applicable if\n",
    "# we use \"partial_fit\"(it is equal 1 Epoch) which is used for Mini Batch Gradient Descent.\n",
    "\n",
    "for _ in range(epochs):\n",
    "    row_indexes = np.random.randint(low = 0, high = x_train.shape[0], size = batch_size) # (batch_size,).\n",
    "    mbgd.partial_fit(X = x_train[row_indexes], y = y_train[row_indexes])\n",
    "\n",
    "print(f\"Coefficients = \\n{mbgd.coef_}, \\n\\nIntercept = {mbgd.intercept_}\\n\")\n",
    "\n",
    "print(f\"Predictions for the First 5 values = \\n{mbgd.predict(x_test[:5])}\\n\")\n",
    "\n",
    "print(f\"r2 score = {r2_score(y_true=y_test, y_pred=mbgd.predict(x_test))}\")\n",
    "\n",
    "explanation = \"\"\"\n",
    "Sklearn's Linear Regression Model returned in above codeblock :   (Compare sklearn's output with our output)\n",
    "----------------------------------------------------------------------------------\n",
    "Coefficients = \n",
    "[  -9.15865318 -205.45432163  516.69374454  340.61999905 -895.5520019\n",
    "  561.22067904  153.89310954  126.73139688  861.12700152   52.42112238],\n",
    "\n",
    "Intercept = 151.88331005254167\n",
    "\n",
    "Predictions for the First 5 values = \n",
    "[154.1213881  204.81835118 124.93755353 106.08950893 258.5348576 ]\n",
    "\n",
    "r2 score = 0.4399338661568968\n",
    "\"\"\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
