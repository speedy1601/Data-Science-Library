{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "329d43f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression, Ridge, Lasso, ElasticNet\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import root_mean_squared_error\n",
    "from sklearn.datasets import load_diabetes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0861ccb4",
   "metadata": {},
   "source": [
    "#                                                   Theory\n",
    "```js\n",
    "        The overall theory that what are these 3 regularizations and their main idea is written succinctly in \"Ridge (L2) and Lasso(L1) and Elasticnet Regularization.pdf\" in Theory folder. If you want to learn deeply from scratch, you may learn them from campusx machine learning tutorial. My main goal is to be pro at Deep Learning, so only got the main idea.\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "557c5989",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For training dataset = 53.04916203242913.\n",
      "For testing  dataset = 55.627840795469155.\n",
      "\n",
      "For Linear = 55.627840795469155.\n",
      "For Ridge regu = 55.627289369073715.\n",
      "For Lasso regu = 55.568975580213966.\n",
      "For ElasticNet regu = 54.964350208376366.\n"
     ]
    }
   ],
   "source": [
    "X, y = load_diabetes(return_X_y = True) # X.shape = (442, 10), Y.shape = (442).\n",
    "\n",
    "# x_train = (353, 10), y_train = (353), x_test = (89, 1), y_test = (89,).\n",
    "x_train, x_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=2) # returns Pandas Dataframes for x_t..\n",
    "                                                                                         # and Series for y_t... .\n",
    "\n",
    "linear = LinearRegression()\n",
    "linear.fit(X = x_train, y = y_train)\n",
    "\n",
    "ridge = Ridge(alpha = 0.00001) # With this alpha value I got the below better result for Ridge.\n",
    "ridge.fit(X = x_train, y = y_train)\n",
    "\n",
    "lasso = Lasso(alpha = 0.01) # With this alpha value I got the below better result for Lasso.\n",
    "lasso.fit(X = x_train, y = y_train)\n",
    "\n",
    "elasticnet = ElasticNet(alpha = 0.001, l1_ratio = 0.6) # l1_ratio is from 0 to 1.\n",
    "elasticnet.fit(X = x_train, y = y_train)\n",
    "\n",
    "#                     Checking overfitting issue with Linear Regression Model.\n",
    "\n",
    "print(f\"For training dataset = {root_mean_squared_error(y_true = y_train, y_pred = linear.predict(x_train))}.\")\n",
    "print(f\"For testing  dataset = {root_mean_squared_error(y_true = y_test,  y_pred = linear.predict(x_test))}.\\n\")\n",
    "\n",
    "#                     There's no overfitting issue but still applied Ridge Regularization for practice purpose.\n",
    "\n",
    "print(f\"For Linear = {root_mean_squared_error(y_true = y_test, y_pred = linear.predict(x_test))}.\")\n",
    "print(f\"For Ridge regu = {root_mean_squared_error(y_true = y_test, y_pred = ridge. predict(x_test))}.\")\n",
    "print(f\"For Lasso regu = {root_mean_squared_error(y_true = y_test, y_pred = lasso. predict(x_test))}.\")\n",
    "print(f\"For ElasticNet regu = {root_mean_squared_error(y_true = y_test, y_pred = elasticnet.predict(x_test))}.\")\n",
    "\n",
    "# Output : ElasticNet Regularization (Linear Regression with Penalty) is winner."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
