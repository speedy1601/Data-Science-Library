{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0eaa67f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.datasets import load_diabetes\n",
    "from sklearn.metrics import r2_score"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "827baca8",
   "metadata": {},
   "source": [
    "#                                              Introduction (Why needed Stochastic Gradient Descent?)\n",
    "```js\n",
    "        The Stochastic Gradient Descent is used here is for Multiple Linear Regression where we have m Input Columns and obviously 1 Output Column. To understand this Gradient Descent, first have a CONSPICUOUS GRASP on 'Batch Gradient Descent' i.e. the Vanilla Gradient Descent.\n",
    "\n",
    "        Purpose :\n",
    "        ---------\n",
    "        Minimizing the Loss Function finding the 'minima'. In our case, minimizing the Linear Regression''s Loss Function (MSE) finding the 'minima'.\n",
    "        \n",
    "        The 'Batch Gradient Descent' algo''s main calculation part for intercept and coefficients :\n",
    "        -------------------------------------------------------------------------------------------\n",
    "\n",
    "        def fit(self, x_train: np.ndarray, y_train: np.ndarray):\n",
    "        B0 = 0 // -> Intercept.\n",
    "        B = np.ones(shape = (x_train.shape[1], 1)) // B1, B2, B3 ..., B10 = [ [1], [1], [1], ..., [1] ] -> Coefficient. (10, 1).\n",
    "        n = x_train.shape[0] // 353, Row numbers.       m = x_train.shape[1] = 10 columns.\n",
    "\n",
    "        for i in range(self.epochs):\n",
    "            y_hat = B0 + np.dot(x_train, B) // (353, 1) shape. y_hat = y_hat_1, y_hat_2 ..., y_hat_m.\n",
    "            slope_B0 = (-2 / n) * np.sum(y_train - y_hat) // Scaler value.\n",
    "            slope_Bs = (-2 / n) * np.dot((y_train - y_hat).T, x_train) // (1, 10) shape.\n",
    "            slope_Bs = slope_Bs.reshape(slope_Bs.shape[::-1]) // (10, 1). B and slope_Bs must be in the same (_, 1) shape.\n",
    "\n",
    "            B0 = B0 - self.learning_rate * slope_B0 // Scaler Value.\n",
    "            B  = B  - self.learning_rate * slope_Bs // (10, 1) shape.\n",
    "\n",
    "        self.intercept_, self.coef_ = B0, B\n",
    "        print(f\"Coefficients = \\n{self.coef_}, \\n\\nIntercept = {self.intercept_}\\n\")\n",
    "\n",
    "        The insights and issues in the above code :\n",
    "        -------------------------------------------\n",
    "        1) In 'Batch Gradient Descent' inside each epoch we update ALL THE COEFFICIENTS(B) with the help of vectorization usingg numpy dot matrix multiplication.\n",
    "                i)   But before updating the coefficients(B) and intercept(B0), we need to calculate the 'y_hat'(y).\n",
    "                ii)  To calculate y_hat we do especially 'np.dot(x_train, B)' i.e. we need to traverse all the rows of x_train to perform multiplication with B. Hold on! We also need to traverse all the rows of y_train in 'slope_B0 and slope_Bs'.\n",
    "                iii) So to update all the coefficients(B1, B2 .. Bm) and intercept(B0) JUST FOR ONE TIME(which occurs in ONE EPOCH), WE NEED TO TRAVERSE ALL THE ROWS OF OUR WHOLE DATASET(x_train, y_train).\n",
    "\n",
    "                iv) So basically 'Batch Gradient Descent' is : For each loop/epoch, traverse all the rows by another loop (i.e. vectorization) and after the nested loop, compute the coefficients and intercept.\n",
    "        \n",
    "        2) But in 'Stochastic Gradient Descent', inside the nested loop (which runs for each row of x_train) we update the coefficients(B1, B2 .. Bm) and intercept(B0) instant for the current row.\n",
    "                i)  Then whats the difference with Batch Gradient Descent here? The difference is, yes we run a nested loop for row_numbers_of_x_train times and inside that loop \"we randomly pick a row_index and update coefficients and intercept only for that random row\".\n",
    "                ii) Why randomly pick a row and update it but not pick row from the first serially? Watch (https://youtu.be/V7KBAa_gh4c?si=xS5UeBNNcjQbxjYf&t=2135).\n",
    "                iii) For 100 epochs, Batch Gradient Descent updates coefficients and intercept for 100 times which works better for small to medium dataset. But for 100 epochs, Stochastic Gradient Descent updates coefficients and intercept for 100(outer loop) * 300(inner loop, assume we have 300 rows) = 3_00000 times which we need for larger dataset.\n",
    "\n",
    "                iii) Since in Stochastic Gradient Descent, for less epochs we are updating the coefficients(B1, B2 .. Bm) and intercept(B0) A LOOOOOOT. That is why for Larger Dataset, Stochastic Gradient Descent helps us to reach the goal point in less time/epochs because for the same amount of updates we need more epochs in Batch Gradient Descent which is slower and impractical.\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba061328",
   "metadata": {},
   "source": [
    "#                                                   Implementation Steps\n",
    "```js\n",
    "        The same steps like 'Batch Gradient Descent' but we now traverse all the rows of the Input Dataset(x_train, y_train) \"MANUALLY\" and update the coefficients and intercept for the randomly_picked_row.\n",
    "\n",
    "        The first rows we need to update of Batch Gradient Descent are :\n",
    "\n",
    "                    for i in range(self.epochs):\n",
    "                        y_hat = B0 + np.dot(x_train, B) // (353, 1) shape. y_hat = y_hat_1, y_hat_2 ..., y_hat_m.\n",
    "                        slope_B0 = (-2 / n) * np.sum(y_train - y_hat) // Scaler value.\n",
    "                        slope_Bs = (-2 / n) * np.dot((y_train - y_hat).T, x_train) // (1, 10) shape.\n",
    "\n",
    "        Changes for Stochastic Gradient Descent :\n",
    "        -----------------------------------------\n",
    "                    B0 = 0 // -> Intercept.\n",
    "                    B = np.ones(shape = (x_train.shape[1])) // B1, B2, B3 ..., B10 = [1, 1, 1, ..., 1] -> Coefficient. (10).\n",
    "\n",
    "                    for i in range(self.epochs):\n",
    "                        for j in range(total_row_numbers):\n",
    "                            row_idx = np.random.randint(low=0, high=total_row_numbers)\n",
    "                            ..........................................................\n",
    "        Now,                  \n",
    "        1) B0 and B will be as it is since we are updating the whole B0 and B for the current randomly_picked_row i.e. 'row_idx'.\n",
    "\n",
    "        2) For 'y_hat' : x_train here will be x_train_for_row_idx i.e. one row and B is also one row. np.dot(one_row, one_row) returns a \"Scaler Value\". So \"y_hat is a scaler value\".\n",
    "\n",
    "                                    y_hat = B0 + np.dot(x_train[row_idx], B) // returns scaler value.\n",
    "\n",
    "        3) For 'slope_B0' : We are updating coefficients and intercept for 1 row i.e. n = 1. y_train here will be y_train_for_row_idx i.e. a scaler value because y_train is 1D Array. So y_train - y_hat returns a scaler value, no need to do 'np.sum'.\n",
    "\n",
    "                                    slope_B0 = -2 * (y_train - y_hat) // returns scaler value.\n",
    "        \n",
    "        4) For 'slope_Bs' : n = 1. (y_train - y_hat) is 'scaler value', so no need to do Transpose. x_train is 1D Array here. \"Between a Scaler Value and 1D Array, np.dot(scaler, 1D_Array) and (scaler * 1D_Array) return same output.\n",
    "\n",
    "                                    slope_Bs = -2 * ((y_train[row_idx] - y_hat) * x_train[row_idx])\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbdbef5d",
   "metadata": {},
   "source": [
    "#                                                     Code Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15ab14d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(353,)\n",
      "Coefficients = \n",
      "[  -9.15865318 -205.45432163  516.69374454  340.61999905 -895.5520019\n",
      "  561.22067904  153.89310954  126.73139688  861.12700152   52.42112238], \n",
      "\n",
      "Intercept = 151.88331005254167\n",
      "\n",
      "Predictions for the First 5 values = \n",
      "[154.1213881  204.81835118 124.93755353 106.08950893 258.5348576 ]\n",
      "\n",
      "r2 score = 0.4399338661568968\n"
     ]
    }
   ],
   "source": [
    "X, Y = load_diabetes(return_X_y=True) # returns np.ndarray. X.shape = (442, 10), Y.shape = (442,).\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(X, Y, test_size=0.2, random_state=2) # (353, 10), (89, 10), (353,), (89,).\n",
    "\n",
    "lr = LinearRegression()\n",
    "lr.fit(X = x_train, y = y_train) # calculated m, b for the future prediction.\n",
    "m, b = lr.coef_, lr.intercept_\n",
    "print(f\"Coefficients = \\n{m.reshape(m.shape[::-1])}, \\n\\nIntercept = {b}\\n\")\n",
    "\n",
    "print(f\"Predictions for the First 5 values = \\n{lr.predict(x_test[:5])}\\n\")\n",
    "\n",
    "print(f\"r2 score = {r2_score(y_true=y_test, y_pred=lr.predict(x_test))}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9de56996",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Coefficients = \n",
      "[   7.74291329 -214.29328175  508.94466724  321.74232391 -121.14533128\n",
      "  -52.63431116 -166.32402329   63.45192667  596.53258923   42.95844928], \n",
      "\n",
      "Intercept = 155.46829576601914\n",
      "\n",
      "Predictions for the First 5 values = \n",
      "[157.38705885 207.04744554 127.35619616 107.13719189 273.00344203]\n",
      "\n",
      "r2 score = 0.44198659984993227\n"
     ]
    }
   ],
   "source": [
    "class GDRegressor:\n",
    "    def __init__(self, learning_rate: float, epochs: int):\n",
    "        self.coef_ = None\n",
    "        self.intercept_ = None\n",
    "        self.lr = learning_rate\n",
    "        self.epochs = epochs\n",
    "    \n",
    "    def fit(self, x_train: np.ndarray, y_train: np.ndarray):\n",
    "        B0 = 0 # -> Intercept.\n",
    "        B = np.ones(shape = (x_train.shape[1])) # B1, B2, B3 ..., B10 = [1, 1, 1, ..., 1] -> Coefficient. (10).\n",
    "        rows = x_train.shape[0] # 353, Total Row Numbers.\n",
    "\n",
    "        for _ in range(self.epochs):\n",
    "            for _ in range(rows):\n",
    "                # Shape : x_train[row_idx] = (10), B = (10), y_train[row_idx] = Scaler value.\n",
    "                row_idx = np.random.randint(low=0, high=rows)\n",
    "                y_hat = B0 + np.dot(x_train[row_idx], B) # y_hat = Scaler Value.\n",
    "                \n",
    "                slope_B0 = -2 * (y_train[row_idx] - y_hat) # Scaler value.\n",
    "                slope_Bs = -2 * (y_train[row_idx] - y_hat) * x_train[row_idx] # (x_train.shape[1]) i.e.\n",
    "                                                                              # (input_column_numbers).\n",
    "                B0 = B0 - self.lr * slope_B0 # Scaler Value.\n",
    "                B  = B  - self.lr * slope_Bs # (input_column_numbers i.e. 353).\n",
    "\n",
    "        self.intercept_, self.coef_ = B0, B\n",
    "        print(f\"Coefficients = \\n{self.coef_}, \\n\\nIntercept = {self.intercept_}\\n\")\n",
    "    \n",
    "    def predict(self, x_test: np.ndarray):\n",
    "        \"\"\"\n",
    "        x_test.shape = (5, 10) and self.coef_.shape = (10). np.dot(x_test.T, self.coef_).shape = (5).\n",
    "        np.dot(x_test, self.coef_) + self.intercept_ = shape (5).\n",
    "        \"\"\"\n",
    "        return np.dot(x_test, self.coef_) + self.intercept_ \n",
    "\n",
    "def main():\n",
    "    gdr = GDRegressor(learning_rate = 0.1, epochs = 100)\n",
    "    gdr.fit(x_train, y_train)\n",
    "\n",
    "    print(f\"Predictions for the First 5 values = \\n{gdr.predict(x_test[:5])}\\n\")\n",
    "\n",
    "    print(f\"r2 score = {r2_score(y_true=y_test, y_pred=gdr.predict(x_test))}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n",
    "\n",
    "explanation = \"\"\"\n",
    "Sklearn returned :\n",
    "------------------\n",
    "Coefficients =\n",
    "[  -9.15865318 -205.45432163  516.69374454  340.61999905 -895.5520019\n",
    "  561.22067904  153.89310954  126.73139688  861.12700152   52.42112238], \n",
    "\n",
    "Intercept = 151.88331005254167\n",
    "\n",
    "Predictions for the First 5 values = \n",
    "[154.1213881  204.81835118 124.93755353 106.08950893 258.5348576 ]\n",
    "\n",
    "r2 score = 0.4399338661568968\n",
    "\n",
    "Note : Since we're picking the row_number randomly, the output varies each time we run it. To optimize the output here, we\n",
    "       use \"Learning Schedule\" which is (we use this \"Learning Schedule\" mostly in Deep Learning)\n",
    "\n",
    "        learning_rate = lambda t: 5 / (t + 50)\n",
    "        for _ in range(self.epochs):\n",
    "            for _ in range(rows):\n",
    "                self.lr = learning_rate(i * x_train.shape[0] + j)\n",
    "                ....................\n",
    "                ....................\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fa74734",
   "metadata": {},
   "source": [
    "#                                                       The Rest Theory\n",
    "```js\n",
    "        Just a fancy tag, nothing else. Must watch [https://youtu.be/V7KBAa_gh4c?si=PCFgVHCgtF781F2w&t=1674] from 27:54 till end.\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7389c5da",
   "metadata": {},
   "source": [
    "#                                       Sklearn's Stochastic Gradient Descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "29175ffb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.datasets import load_diabetes\n",
    "from sklearn.metrics import r2_score\n",
    "\n",
    "from sklearn.linear_model import SGDRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "480960c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Coefficients = [  60.86961237  -42.20575615  303.18632103  218.89600798   31.17966896\n",
      "   -5.58495197 -158.80328801  128.3211942   279.31828638  126.17902842]\n",
      "Intercept = [150.98436032].\n",
      "\n",
      "r2 score = 0.4165326502819473.\n",
      "\n",
      "Prediction for the first 5 values = [153.18706955 186.97492584 141.9537144  111.13859457 234.14998623].\n"
     ]
    }
   ],
   "source": [
    "X, Y = load_diabetes(return_X_y=True) # returns np.ndarray. X.shape = (442, 10), Y.shape = (442,).\n",
    "x_train, x_test, y_train, y_test = train_test_split(X, Y, test_size=0.2, random_state=2) # (353, 10), (89, 10), (353,), (89,).\n",
    "\n",
    "sgd = SGDRegressor(loss = 'squared_error', max_iter = 111, learning_rate = 'constant', eta0 = 0.01)\n",
    "\"\"\"\n",
    "loss = In our case, we are optimizing the Linear Regression's Loss Function which is MSE (Mean Squared Error).\n",
    "max_iter = Maximum Epoch.\n",
    "learning_rate = Want to keep the Initial Learning Rate constant or not.\n",
    "eta0 = Initial Learning Rate.\n",
    "\"\"\"\n",
    "\n",
    "sgd.fit(X = x_train, y = y_train)\n",
    "\n",
    "print(f\"Coefficients = {sgd.coef_}\\nIntercept = {sgd.intercept_}.\\n\")\n",
    "\n",
    "print(f\"r2 score = {r2_score(y_true = y_test, y_pred = sgd.predict(x_test))}.\\n\")\n",
    "\n",
    "print(f\"Prediction for the first 5 values = {sgd.predict(x_test[:5])}.\")\n",
    "\n",
    "warning = \"\"\"\n",
    "If we get the error \"ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing\n",
    "max_iter to improve the fit.\", then need to increase the max_iter.\n",
    "\"\"\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
