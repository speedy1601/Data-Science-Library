{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "684bab00",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.datasets import make_regression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.datasets import load_diabetes\n",
    "from sklearn.metrics import r2_score"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11e5eb3d",
   "metadata": {},
   "source": [
    "#                                                   Introduction\n",
    "```js\n",
    "        The Gradient Descent is used here is for Multiple Linear Regression where we have m Input Columns and obviously 1 Output Column.\n",
    "\n",
    "        The equations and intutions are hand written from scratch in \"Gradient Descent for Multiple LR.pdf\".\n",
    "\n",
    "        NOTE : This is also called \"Batch Gradient Descent\", \"Vanilla Gradient Descent\".\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "39e7413a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Coefficients = \n",
      "[[  -9.15865318]\n",
      " [-205.45432163]\n",
      " [ 516.69374454]\n",
      " [ 340.61999905]\n",
      " [-895.5520019 ]\n",
      " [ 561.22067904]\n",
      " [ 153.89310954]\n",
      " [ 126.73139688]\n",
      " [ 861.12700152]\n",
      " [  52.42112238]], \n",
      "\n",
      "Intercept = [151.88331005]\n",
      "\n",
      "Predictions for the First 5 values = \n",
      "[[154.1213881 ]\n",
      " [204.81835118]\n",
      " [124.93755353]\n",
      " [106.08950893]\n",
      " [258.5348576 ]]\n",
      "\n",
      "r2 score = 0.4399338661568968\n"
     ]
    }
   ],
   "source": [
    "X, Y = load_diabetes(return_X_y=True) # returns np.ndarray. X.shape = (442, 10), Y.shape = (442,).\n",
    "Y = Y.reshape((Y.shape[0], 1))        # Y.shape = (442, 1).\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(X, Y, test_size=0.2, random_state=2) # (353, 10), (89, 10), (353, 1), (89, 1)\n",
    "\n",
    "lr = LinearRegression()\n",
    "lr.fit(X = x_train, y = y_train) # calculated m, b for the future prediction.\n",
    "m, b = lr.coef_, lr.intercept_\n",
    "print(f\"Coefficients = \\n{m.reshape(m.shape[::-1])}, \\n\\nIntercept = {b}\\n\")\n",
    "\n",
    "print(f\"Predictions for the First 5 values = \\n{lr.predict(x_test[:5])}\\n\")\n",
    "\n",
    "print(f\"r2 score = {r2_score(y_true=y_test, y_pred=lr.predict(x_test))}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f36a4f0d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Coefficients = \n",
      "[[  -9.15830466]\n",
      " [-205.45292726]\n",
      " [ 516.69716303]\n",
      " [ 340.61931932]\n",
      " [-895.3830603 ]\n",
      " [ 561.08657372]\n",
      " [ 153.81793572]\n",
      " [ 126.70993046]\n",
      " [ 861.06383   ]\n",
      " [  52.4214399 ]], \n",
      "\n",
      "Intercept = 151.8833119378528\n",
      "\n",
      "Predictions for the First 5 values = \n",
      "[[154.121059  ]\n",
      " [204.8186888 ]\n",
      " [124.93735432]\n",
      " [106.08877848]\n",
      " [258.53753532]]\n",
      "\n",
      "r2 score = 0.43993516054291604\n"
     ]
    }
   ],
   "source": [
    "#   Here I calculated 'B0 and B' using Gradient Descent from the Error/Loss Function and USING EPOCH i.e. TILL SPECIFIC LIMIT.\n",
    "\n",
    "class GDRegressor:\n",
    "    def __init__(self, learning_rate: float, epochs: int):\n",
    "        self.coef_ = None\n",
    "        self.intercept_ = None\n",
    "        self.learning_rate = learning_rate\n",
    "        self.epochs = epochs\n",
    "    \n",
    "    def fit(self, x_train: np.ndarray, y_train: np.ndarray):\n",
    "        B0 = 0 # -> Intercept.\n",
    "        B = np.ones(shape = (x_train.shape[1], 1)) # B1, B2, B3 ..., B10 = [ [1], [1], [1], ..., [1] ] -> Coefficient. (10, 1).\n",
    "        n = x_train.shape[1] # 10, Column numbers.\n",
    "\n",
    "        for i in range(self.epochs):\n",
    "            y_hat = B0 + np.dot(x_train, B) # (353, 1) shape.\n",
    "            slope_B0 = (-2 / n) * np.sum(y_train - y_hat) # Scaler value.\n",
    "            slope_Bs = (-2 / n) * np.dot((y_train - y_hat).T, x_train) # (1, 10) shape.\n",
    "            slope_Bs = slope_Bs.reshape(slope_Bs.shape[::-1]) # (10, 1). B and slope_Bs must be in the same (_, 1) shape.\n",
    "\n",
    "            B0 = B0 - self.learning_rate * slope_B0 # Scaler Value.\n",
    "            B  = B  - self.learning_rate * slope_Bs # (10, 1) shape.\n",
    "\n",
    "        self.intercept_, self.coef_ = B0, B\n",
    "        print(f\"Coefficients = \\n{self.coef_}, \\n\\nIntercept = {self.intercept_}\\n\")\n",
    "    \n",
    "    def predict(self, x_test: np.ndarray):\n",
    "        \"\"\"\n",
    "        The main equation is np.dot(self.coef_, x_test) + self.intercept_.\n",
    "\n",
    "        But here self.coef_.shape = (10, 1). x_test.shape (5, 10). But we need to put the column numbers which is constant(10)\n",
    "        side by side i.e. (1, 10) (10, 5) since row numbers can be different but not column numbers.\n",
    "\n",
    "        (1, 10) (10, 5) = (1, 5) but I want to visualize it to (5, 1) shape. So I did \".T\" on the whole output.\n",
    "        \"\"\"\n",
    "        return ( np.dot(self.coef_.T, x_test.T) + self.intercept_ ).T\n",
    "\n",
    "def main():\n",
    "    gdr = GDRegressor(learning_rate = 0.02, epochs = 300000) # 0.02, 300000\n",
    "    gdr.fit(x_train, y_train)\n",
    "\n",
    "    print(f\"Predictions for the First 5 values = \\n{gdr.predict(x_test[:5])}\\n\")\n",
    "\n",
    "    print(f\"r2 score = {r2_score(y_true=y_test, y_pred=gdr.predict(x_test))}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n",
    "\n",
    "explanation = \"\"\"\n",
    "At above codeblock, sklearn returned us :\n",
    "\n",
    "Coefficients = \n",
    "[[  -9.15865318]\n",
    " [-205.45432163]\n",
    " [ 516.69374454]\n",
    " [ 340.61999905]\n",
    " [-895.5520019 ]\n",
    " [ 561.22067904]\n",
    " [ 153.89310954]\n",
    " [ 126.73139688]\n",
    " [ 861.12700152]\n",
    " [  52.42112238]], \n",
    "\n",
    "Intercept = [151.88331005]\n",
    "\n",
    "Predictions for the First 5 values = \n",
    "[[154.1213881 ]\n",
    " [204.81835118]\n",
    " [124.93755353]\n",
    " [106.08950893]\n",
    " [258.5348576 ]]\n",
    "\n",
    "r2 score = 0.4399338661568968\n",
    "\n",
    "This output and our output are same except some difference in decimals.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2149d5ca",
   "metadata": {},
   "source": [
    "#                                               Cons of Batch Gradient Descent\n",
    "```js\n",
    "        for i in range(self.epochs):\n",
    "            y_hat = B0 + np.dot(x_train, B) // (353, 1) shape.\n",
    "            slope_B0 = (-2 / n) * np.sum(y_train - y_hat) // Scaler value.\n",
    "            slope_Bs = (-2 / n) * np.dot((y_train - y_hat).T, x_train) // (1, 10) shape.\n",
    "\n",
    "            B0 = B0 - self.learning_rate * slope_B0 // Scaler Value.\n",
    "            B  = B  - self.learning_rate * slope_Bs // (10, 1) shape.\n",
    "        \n",
    "    Cons of Batch Gradient Descent :\n",
    "    --------------------------------\n",
    "    All those operations are Vectorization, means e.g. to calculate y_hat we put the whole x_train into memory and do dot product between x_train and B. If x_train is large e.g. 10_000 rows with 50 input columns, then putting such a huge dataset in the memory entirely may give us a \"memory out error\". We are again doing dot product with x_train in 'slope_Bs'. So we are doing a lot computation in each epoch/iteration. So we need significant memory for Batch Gradient Descent if the dataset is large.\n",
    "\n",
    "    Addressing these issues, there exists Stochastic Gradient Descent.\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
